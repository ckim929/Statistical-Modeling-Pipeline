---
title: Feature Engineering
author: Cathy Kim
date: 2023-02-03
category: Jekyll
layout: post
---

What is **feature engineering** and why is it important?

A *feature* represents any measurable input that can be used for analysis. 

*Feature engineering* takes raw observations and converts them into useful features for use in supervised learning. It is an important step in the data science pipeline because it has a direct impact on being able to create models with better accuracy. 

Feature engineering consists of various processes, including feature creation, transformations, extractions, EDA, and Benchmark. 
- *Feature creation* involves adding or removing features to utilize features that will be most helpful for the model.
- *Transformations* involve taking a function to convert a feature from one representation to another.
- *Feature extraction* involves extracting features from a dataset without distorting its original relationships in order to identify useful information that is contained.
- *EDA*, shown in the previous blog post, is used to better understand the data by exploring its properties. 
- *Benchmark* is the step in which you measure how your model against a known dependable model to compare its performance ([Towards Data Science](https://towardsdatascience.com/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10))

For this credit card dataset, there is not any more feature engineering to be done because the dataset already represents the important, compressed data as a result of a **principal component analysis (PCA)** transformation.

### Principal Component Analysis (PCA) Transformation

The [credit card dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) has already gone through a PCA transformation and been extracted and reduced down to 28 features. 'Time' and 'Amount' are the only features that has not been transformed.

A PCA transformation is a dimensionality-reduction method that transforms a large set of variables into a smaller one while still keeping most of the original information. Essentially, it reduces the number of variables while preserving as much information as possible ([builtin](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)). PCA helps reduce the number of columns and makes them uncorrelated.

Wikipedia defines PCA as

> an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

See this post by [builtin](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) for more mathematical details (Jaddi).


### Binning and Transformation of Features
Binning involves dividing numerical features into distinct groups. This step is important in order to be able to identify and engineer features that better emphasize important trends in the data.

I will be defining bins with `pd.qcut` which is a quantile-based discretization function. I will be using 11 bins and visualizing all of the features to be able to determine which predictors I should apply transformations to. 

![lineplots_code](https://user-images.githubusercontent.com/86743951/224512939-eab488fe-1d35-46ee-b33a-9e761cf5db75.png)

![savelineplots](https://user-images.githubusercontent.com/86743951/224512869-5e703ece-8a33-4d35-b958-d283bbf949ce.png)


