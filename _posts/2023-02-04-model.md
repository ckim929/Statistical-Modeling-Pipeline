---
title: Modeling
author: Cathy Kim
date: 2023-02-04
category: Jekyll
layout: post
---

This step is where the "learning" will begin, a.k.a. building models. The model built using the training data will then be used to predict the output value. Over several iterations, the model will be improved for optimal prediction accuracy. The model's performance will be evaluated against the test data using the appropriate metrics. 

This credit card dataset is a very imbalanced dataset, so using most default machine learning algorithms would pose a problem, as most algorithms prioritize the maximization of accuracy. 

Accuracy is *not* the metric I want to use because it is a very misleading metric in an imbalanced classification problem. 
Therefore, the performance metrics I will be using to evaluate my models are confusion matrices and sklearn's classification report (precision, recall, f1-score, support). 

### Metrics

A **confusion matrix** is a summary showing the number of correct and incorrect predictions broken down by class. In this case of credit card fraud detection, it would show the true negatives (correctly identified as not being fraud), false positives (incorrectly identified as being fraud), false negatives (incorrectly identified as not being fraud), and true positives (correctly identified as being fraud). In this credit card classification problem, the objective would be to minimize both the number of false positives and false negatives while prioritizing the minimization of false negatives because incorrectly identifying a fraudulent transaction as being not fraudulent would have worse consequences than incorrectly identifying a non-fraudulent transaction as being fraudulent. 

![confusion-matrix](https://user-images.githubusercontent.com/86743951/222971527-3b6fa067-0b41-490c-8df2-4165b7b2d008.jpg)

**Precision** is the percentage of correct positive predictions in relation to the number of total positive predictions. It is useful for determining when the costs of false positives is high, which in this case of credit card fraud detection would be incorrectly classifying a non-fraudulent transaction as being one of fraud. The cardholder may run into difficulties using their card if it gets marked as fraud even when it is a legitimate transaction.

![precision](https://user-images.githubusercontent.com/86743951/222974356-496aae09-deec-420f-b6b4-7c8e7dd31d3c.jpg)


**Recall** is the percentage of correct positive predictions in relation to the number of true positives and false negatives. Recall is useful for determining when there is a high cost associated with false negatives. In this case, it would be classifying a fraudulent transaction as being non-fraudulent. This is an important metric to prioritize as predicting a fraudulent transaction as being non-fraudulent would have bad consequences. Between precision and recall, recall would be the metric I want to prioritize in improving.

![recall](https://user-images.githubusercontent.com/86743951/222974367-4d084c21-cf7b-4276-af09-c02882c2b046.jpg)


The **F1 Score** combines the precision and recall into one metric by averaging the two together. This metric strikes a balance between both precision and recall.

![f1score](https://user-images.githubusercontent.com/86743951/222974380-e7aba3c5-fbf3-46cd-b1c8-493215b17820.jpg)


### So how do we counteract issues with class imbalance?

The common methods to counteract class imbalance are *oversampling* and *undersampling*.

**Oversampling** creates duplicates of the data that is least present in the dataset and adds it to the dataset. While this method does not require deletion of data points, it leaves room for the possibility of providing the model false information. Several identical values in a dataset leaves room for possible model performance issues. 

**Undersampling** deletes data points that are more prevalent in the dataset. This poses potential problems as large amounts of data must be deleted. The larger the class imbalance, the more data that would be deleted.


I will be using an improved method called **Synthetic Minority Oversampling Technique (SMOTE)** which is an algorithm that creates synthetic data points that are slightly different from the original data points. It is essentially a fancy, more improved version of oversampling. Rather than duplicates of existing data, the SMOTE algorithm uses a random sample from the minority class and uses k-nearest neighbors to identify the vector between the selected neighbor and the data point. Multiplying the vector by a random number between 0 and 1 results in a synthetic data point that is then added to the current data point. 

The paper that introduced SMOTE is linked [here](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html) (Chawla, et al).


## Base Models
It is important to have a base model so that it can serve as a benchmark for the performances when using SMOTE.
First, Using [Scikit Learn's Logistic Regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), I will build two models on the original data without using SMOTE: one with default `class_weight` and one with `class_weight = 'balanced'`. 

Without the specification of `class_weight`, the model assumes the default, meaning all classes have weight one. With an imbalanced dataset, consequently, the algorithm will be biased towards predicting the majority class.

With the specification of `class_weight = 'balanced'`, the model takes the *y* values and automatically adjusts the weights to be inversely proportional to the class frequencies in the input data. This gives lower weights to the majority class and higher weights to the minority class.

#### Default Logistic Regression Model

I set `random_state = 1` to set a seed to the random generator to fix the outcome and set `max_iter = 1000` to specify the maximum number of iterations. Setting the maximum iteration too high may increase the training set accuracy but result in overfitting. 

![log_not_balanced](https://user-images.githubusercontent.com/86743951/222561423-70ed48a4-7a4e-4171-ab14-963630b56515.png)

There are 11 false positives and 49 false negatives meaning the model incorrectly classified 11 non-fraudulent transactions as being fraudulent and 49 fraudulent trasactions as being non-fraudulent. False positives and false positives are both wrong predictions, so I wnat to minimize them. However, in the case of fraud detection, having false negatives is much worse than having false positives. Due to the usage of biased class data, the model was not very good at identifying fraudulent transactions.


#### Balanced Class Logistic Regression Model

Here, I will set `class_weights = 'balanced'` to take into account the skewed distribution. By specifying this in the settings, the model assigns the class weights inversely proportional to their frequencies.

![bal_log_model](https://user-images.githubusercontent.com/86743951/222983222-bb95e733-91de-485e-8fd2-eae43ecd551c.png)

Comparing these results to the results from the default logistic regression model, it can be observed that the number of false negatives decreased from 49 to 14, which is an improvement. However, the number of false positives increased drastically from 11 to 1536. 

## SMOTE
SMOTE stands for **Synthetic Minority Oversampling Technique**. SMOTE is a Machine Learning technique that better deals with imbalanced data in classification problems. 

To use **SMOTE**, I will be importing ```SMOTE``` from the ```imblearn``` library.

![image](https://user-images.githubusercontent.com/86743951/222567663-5fae926b-f4c5-42cf-a70c-7a64c1b35855.png)

Using the ```SMOTE``` function, I created resample datasets of x and y called x_resampled and y_resampled, respectively.

![resampled_var](https://user-images.githubusercontent.com/86743951/222563661-e34894e7-0b1a-4964-b90b-a56598308810.png)

I visualized the class distribution like I had done with the [original data](https://ckim929.github.io/datascience/jekyll/2023-02-02-eda.html?h=Splitting%20the%20dataset%20into%20train%20and%20test%20data). 

Compared to the imbalance present in the original data, we can see that there are a lot more data points added to 'Class = 1'. 

![smote_class_bar](https://user-images.githubusercontent.com/86743951/222565774-a49c4ca0-5786-498a-82c0-878ae7ab5faa.png)

![smote_model](https://user-images.githubusercontent.com/86743951/222983238-9975d3d8-0c78-48f9-b150-64cd0c7537ae.png)

** MORE TO BE ADDED **






