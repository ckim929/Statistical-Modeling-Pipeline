---
title: Modeling
author: Cathy Kim
date: 2023-02-04
category: Jekyll
layout: post
---

** Introduction to modeling here **

This credit card dataset is a very imbalanced dataset, so using most default machine learning algorithms would pose a problem, as most algorithms prioritize the maximization of accuracy. 

Accuracy is *not* the metric I want to use because it is a very misleading metric in an imbalanced classification problem. 
Therefore, the performance metrics I will be using to evaluate my models are confusion matrices and sklearn's classification report (precision, recall, f1-score, support). 

### Metrics

A **confusion matrix** is a summary showing the number of correct and incorrect predictions broken down by class. In this case of credit card fraud detection, it would show the true negatives (correctly identified as not being fraud), false positives (incorrectly identified as being fraud), false negatives (incorrectly identified as not being fraud), and true positives (correctly identified as being fraud). In this credit card classification problem, the objective would be to minimize both the number of false positives and false negatives while prioritizing the minimization of false negatives because incorrectly identifying a fraudulent transaction as being not fraudulent would have worse consequences than incorrectly identifying a non-fraudulent transaction as being fraudulent. 

![confusion-matrix](https://user-images.githubusercontent.com/86743951/222971527-3b6fa067-0b41-490c-8df2-4165b7b2d008.jpg)

**Precision** is the percentage of correct positive predictions in relation to the number of total positive predictions.

![precision](https://user-images.githubusercontent.com/86743951/222973674-e52b64a9-f80f-494f-8978-8d93cffd601e.jpg)


**Recall** is the percentage of correct positive predictions in relation to the number of true positives and false negatives.

![recall](https://user-images.githubusercontent.com/86743951/222973695-6e8f445d-05be-4124-888f-438c2c9820f2.jpg)


The **F1 Score** combines the precision and recall into one metric by averaging the two together. 

![f1score](https://user-images.githubusercontent.com/86743951/222973712-ad6c41f8-63a8-47ec-a1c2-17318d6af865.jpg)


### So how do we counteract issues with class imbalance?

The common methods to counteract class imbalance are *oversampling* and *undersampling*.

**Oversampling** creates duplicates of the data that is least present in the dataset and adds it to the dataset. While this method does not require deletion of data points, it leaves room for the possibility of providing the model false information. Several identical values in a dataset leaves room for possible model performance issues. 

**Undersampling** deletes data points that are more prevalent in the dataset. This poses potential problems as large amounts of data must be deleted. The larger the class imbalance, the more data that would be deleted.


I will be using a method called **Synthetic Minority Oversampling Technique (SMOTE)** which is an algorithm that creates synthetic data points that are slightly different from the original data points. It is essentially a fancy, more improved version of oversampling. Rather than duplicates of existing data, the SMOTE algorithm uses a random sample from the minority class and uses k nearest neighbors to identify the vector between the selected neighbor and the data point. Multiplying the vector by a random number between 0 and 1 results in a synthetic data point that is then added to the current data point. 

The paper that introduced SMOTE is linked [here](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html).


## Base Model
It is important to have a base model so that it can serve as a benchmark for the performances when using SMOTE.
First, Using [Scikit Learn's Logistic Regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), I will build two models on the original data without using SMOTE: one with default `class_weight` and one with `class_weight = 'balanced'`. 

Without the specification of `class_weight`, the model assumes the default, meaning all classes have weight one. With an imbalanced dataset, consequently, the algorithm will be biased towards predicting the majority class.

With the specification of `class_weight = 'balanced'`, the model takes the y values and automatically adjusts the weights to be inversely proportional to the class frequencies in the input data. This gives lower weights to the majority class and higher weights to the minority class.

#### Default Logistic Regression Model
![log_not_balanced](https://user-images.githubusercontent.com/86743951/222561423-70ed48a4-7a4e-4171-ab14-963630b56515.png)

#### Balanced Class Logistic Regression Model
![bal_log_model](https://user-images.githubusercontent.com/86743951/222562080-32a4bf69-9738-4169-8dd9-d4cf3164e8ef.png)

**INTERPRET RESULTS HERE**

## SMOTE
SMOTE stands for **Synthetic Minority Oversampling Technique**. SMOTE is a machine learning technique that better deals with imbalanced data in classification problems. 

To use **SMOTE**, I will be importing ```SMOTE``` from the ```imblearn``` library.

![image](https://user-images.githubusercontent.com/86743951/222567663-5fae926b-f4c5-42cf-a70c-7a64c1b35855.png)

Using the ```SMOTE``` function, I created resample datasets of x and y called x_resampled and y_resampled, respectively.

![resampled_var](https://user-images.githubusercontent.com/86743951/222563661-e34894e7-0b1a-4964-b90b-a56598308810.png)

I visualized the class distribution like I had done with the [original data](https://ckim929.github.io/datascience/jekyll/2023-02-02-eda.html?h=Splitting%20the%20dataset%20into%20train%20and%20test%20data). 

Compared to the imbalance present in the original data, we can see that there are a lot more data points added to 'Class = 1'. 

![smote_class_bar](https://user-images.githubusercontent.com/86743951/222565774-a49c4ca0-5786-498a-82c0-878ae7ab5faa.png)

![smote_model](https://user-images.githubusercontent.com/86743951/222566169-03d57cdd-eda5-4d6d-b8f0-07e6fc0065da.png)

**MORE TO BE ADDED**






