---
title: EDA
author: Cathy Kim
date: 2023-02-02
category: Jekyll
layout: post
---

## Splitting Data | Data Cleaning & Exploratory Data Analysis (EDA)

![EDA](https://user-images.githubusercontent.com/86743951/218554426-a54bcb7d-68b8-4a77-951d-d02ce1a76642.png)

The first step of the data science pipeline is **data cleaning**. This process involves fixing or removing incorrect, duplicate, or incomplete data within a dataset. The dataset I am working with contains only numerical variables resulting from a principal component analysis (PCA) transformation.

It is important to comprehensively understand the data to be able to gather as much insight as possible. The goal of **exploratory data analysis (EDA)** is to make sense of the data by looking for patterns, spotting anomalies, and checking assumptions through summary statistics and visualizations ([towardsdatascience](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)). There is a lot of overlap between the processes of data cleaning and performing EDA.

I first imported the necessary packages: pandas, numpy, seaborn, and matplotlib.

**Code block of importing packages to be added**

After reading in the data using panda's ```.read_csv()``` function, I used the ```.head()``` function of the pandas library to look at the first five observations of my dataset. This ```.head()``` function is useful for getting a quick glance of whether the dataset encompasses the right type of data.

![readindata](https://user-images.githubusercontent.com/86743951/219407479-0102dc03-e4f9-48ed-bd6c-def8b2bc72ab.png)

![head](https://user-images.githubusercontent.com/86743951/214944882-7877bba7-3458-4de6-8bd7-3e98fd4761cb.png)

I then used the ```.shape``` function of the pandas library to return the shape of the dataframe. We can see that there are 284807 rows and 31 columns.

![datashape](https://user-images.githubusercontent.com/86743951/218572644-afc75098-b765-4651-ae9b-c4cf47759559.png)

The ```.info()``` function is also useful pandas function for printing important information such as its index dtype and columns, non-null values and memory usage.

![datainfo](https://user-images.githubusercontent.com/86743951/215006655-682f7a9b-632b-4d45-895e-bbef744514e1.png)

#### Checking for Null Values
To understand if I need to perform data cleaning and remove or impute values, I used the ```is.null().sum()``` function of the pandas library to see if I had any null values in the datset.

![null_values](https://user-images.githubusercontent.com/86743951/215003746-1351832a-c590-4fb7-b0e0-723957f149c6.png)

The dataset does not contain any null values.

#### Checking for Duplicate Rows
I then used the ```.duplicated()``` to see if there existed any duplicate rows in my dataset. Using the ```.duplicated()``` function of the pandas library returned a boolean series denoting duplicate rows. 

![duplicates](https://user-images.githubusercontent.com/86743951/215144874-93d46864-71d0-40a3-ad48-e42dd7f54fac.png)

I found that my dataset contains 1081 duplicated rows.
As this dataset is a result of a PCA transformation, I don't have the full picture of how this data was gathered and what the features represent. With the 28 different features, amount, and time variables, I would infer that it would be rare for two transactions to have the *exact* same values. With the little information about the dataset given, I don't have reason to believe that these duplicates could be the actual nature of the dataset. 

Duplicate data points in the dataset may lead to bias in the model and overfitting by adding the weights of the samples. For this reason, I dropped the duplicated rows using the pandas ```drop_duplicates()``` function.

![drop_duplicates](https://user-images.githubusercontent.com/86743951/215145698-ac4c8bfd-4eff-4def-aee9-c8dcf5c436eb.png)

The original dataframe contained 284807 rows, but after removing the duplicated rows, the number of rows reduced to 283726 rows. 

![shape](https://user-images.githubusercontent.com/86743951/218567242-10d98d9a-3f6c-4c08-8096-ad15dd5ab926.png)

#### Splitting the dataset into train and test data
At this point, I split the dataset into train and test data. The train dataset will be used to train the model and the test dataset will be used to evaluate the model. I will utilize the **train_test_split** package imported from sklearn.model_selection to split the train and test data into 75% and 25%, respectively.

![imbalance](https://user-images.githubusercontent.com/86743951/219413476-8e7c00eb-4e7f-454d-a81a-248efaaa2e88.png)

Due to the large imbalance exhibited in the distribution of the target class, I used stratified sampling to ensure that fraud and non-fraud cases both exist in the train and test data. I did this by using the parameter *stratify* in sckikit-learn's train_test_split function. 

![traintestsplit](https://user-images.githubusercontent.com/86743951/219413937-7e8ec37b-d37d-4962-977d-912d723cb555.png) 

I then concatenated the ```x_train``` and ```y_train``` datasets to make correlation to conduct further EDA.

![concat](https://user-images.githubusercontent.com/86743951/219414603-5fe31aff-3dfa-419f-949e-ca159dcb67d9.png)

It is important to do EDA *only* on the training data to keep an unbiased estimate of the model's performance. Using the testing data for EDA runs the risk of engineering features that work well for the testing data, resulting in overfitting.

### Making visualizations

#### Correlation

Correlation is a measure of the relationship between two variables. It aids in the understanding of the extent certain features influence whether a specific transaction is fraud or not.
The correlation coefficient ranges from -1 to +1, where -1 decribes a perfect negative correlation and +1 describes a perfect positive (direct) correlation. 

Below, I've visualized the train dataset correlation in two different ways: a heat map using seaborn, and a bar chart using matplotlib. 

![traincorr](https://user-images.githubusercontent.com/86743951/219416190-d09c0636-fb0d-47b7-afed-e33aa85e48ea.png)

![heatmap](https://user-images.githubusercontent.com/86743951/219416980-d8b37a16-5bb6-476b-9be6-1d4e053c928c.png)

![barchart](https://user-images.githubusercontent.com/86743951/219417177-e56b318a-d0d5-4377-9a01-a13ed59c0221.png)

#### Boxplots

Boxplots give us a way to visualize the distribution of the features in fraudulent and non fraudulent transactions. 

![boxplotcode](https://user-images.githubusercontent.com/86743951/221439661-74c4a192-83b8-49a5-8cb9-3d540d507753.png)

![saveboxplots](https://user-images.githubusercontent.com/86743951/221444313-6d118dbe-c432-4a65-8b5b-a8376d6881d6.png)


#### Data binning

Binning is a method to group a number of continuous or categorical values into a smaller number of bins. 
Data binning can help with interpreting data and reducing noise.

**More to be added**


